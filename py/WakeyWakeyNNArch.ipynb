{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WakeyWakeyNNArch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2hKjshaHD11m",
        "xg7bfTF1bBVe",
        "haiPVx4ibEra",
        "1jTOL7scbMs7",
        "dDriUNRThyQF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c98c548bf0b244a19bea2ce61d421bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4f000045eb0544acb018018b62ef86a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f9c3e10e56a94b1389d625be5f2fab45",
              "IPY_MODEL_bc57fadc20b343c08db45ac414e227a4"
            ]
          }
        },
        "4f000045eb0544acb018018b62ef86a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9c3e10e56a94b1389d625be5f2fab45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1692f6090fec4faf84c844dac49abd21",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6036,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6036,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1401ca2ab069455eb3d0ad55e6948f86"
          }
        },
        "bc57fadc20b343c08db45ac414e227a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc84e3c533c04d69bba5ed9fbdc67216",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6036/6036 [03:07&lt;00:00, 32.15it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf94dfb385334be9a33f1a09b1ad310a"
          }
        },
        "1692f6090fec4faf84c844dac49abd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1401ca2ab069455eb3d0ad55e6948f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc84e3c533c04d69bba5ed9fbdc67216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf94dfb385334be9a33f1a09b1ad310a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuhU8wRTOAQW"
      },
      "source": [
        "# Featurization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWEcSFPLm18V"
      },
      "source": [
        "When training, make sure to set your runtime type to GPU by going to `Runtime > Change runtime type` and selecting `GPU` from the dropdown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_WLB-Rm1xMJ",
        "outputId": "6d0d6e1d-4091-49ea-8b08-6343787d5816"
      },
      "source": [
        "import os\n",
        "# download and unzip keywords dataset\n",
        "os.system('curl -O https://cdn.edgeimpulse.com/datasets/keywords2.zip')\n",
        "os.system('unzip keywords2.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "c98c548bf0b244a19bea2ce61d421bea",
            "4f000045eb0544acb018018b62ef86a0",
            "f9c3e10e56a94b1389d625be5f2fab45",
            "bc57fadc20b343c08db45ac414e227a4",
            "1692f6090fec4faf84c844dac49abd21",
            "1401ca2ab069455eb3d0ad55e6948f86",
            "fc84e3c533c04d69bba5ed9fbdc67216",
            "cf94dfb385334be9a33f1a09b1ad310a"
          ]
        },
        "id": "0JyIhhVnzph4",
        "outputId": "6b131155-fcab-40e0-8193-36ffedbc3ecd"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "\"\"\"\n",
        "ACO (Acoustic Featurization) Software Model\n",
        "\n",
        "This is used to test our custom acoustic featurization pipeline against\n",
        "the speechpy implementation used in EdgeImpulse.\n",
        "\"\"\"\n",
        "!pip install speechpy\n",
        "import os\n",
        "import speechpy\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from scipy.fft import dct\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "train_test_split = 0.75  # fraction to have as training data\n",
        "\n",
        "# ==================== DFE modelling ====================\n",
        "\n",
        "ratio_in = 250\n",
        "ratio_out = 250\n",
        "\n",
        "def shift_zero_to_one(x):\n",
        "    '''Shift a 16b signed input signal into the range 0-1.'''\n",
        "    x = x.astype(np.float64)\n",
        "    x = x + 2**15\n",
        "    x = x / 2**16\n",
        "    return x\n",
        "\n",
        "def pcm_to_pdm_pwm(x):\n",
        "    '''Pack all ones and zeros together during upsampling when generating PDM\n",
        "    signal, effectively creating PWM. This results in much less high frequency\n",
        "    noise in the signal.'''\n",
        "    x = shift_zero_to_one(x)\n",
        "    # added\n",
        "    x -= 0.5\n",
        "    x /= 13\n",
        "    x += 0.5\n",
        "    # end\n",
        "    x = x * ratio_in  # scale up by window so that value is # ones\n",
        "    x = x.astype(np.uint8)\n",
        "    repeats = np.zeros(x.size * 2, dtype=np.uint8)\n",
        "    repeats[::2] = x\n",
        "    repeats[1::2] = ratio_in - x\n",
        "    one_zero = np.ones(x.size * 2, dtype=np.uint8)\n",
        "    one_zero[1::2] = 0\n",
        "    y = np.repeat(one_zero, repeats)\n",
        "    return y\n",
        "\n",
        "def cic1(x):\n",
        "    '''First cic stage treats datatypes differently.'''\n",
        "    x = x.astype(np.int64)\n",
        "    rolled = np.roll(x, ratio_out)\n",
        "    rolled[:ratio_out] = 0\n",
        "    x = np.cumsum(x) - np.cumsum(rolled)\n",
        "    x = x - int(ratio_out/2)\n",
        "    x = x.astype(np.int8)\n",
        "    # x = x.astype(np.int16)  # if ratio is >= 256, need this to not overflow\n",
        "    return x\n",
        "\n",
        "def cicn(x):\n",
        "    '''Later CIC stages.'''\n",
        "    x = x.astype(np.int64)\n",
        "    rolled = np.roll(x, ratio_out)\n",
        "    rolled[:ratio_out] = 0\n",
        "    x = np.cumsum(x) - np.cumsum(rolled)\n",
        "    x = x / ratio_out\n",
        "    x = x.astype(np.int8)\n",
        "    # x = x.astype(np.int16)  # same here\n",
        "    return x\n",
        "\n",
        "def pcm_to_pdm(x, pdm_gen='err'):\n",
        "    '''Generate the PDM signal for the sample.'''\n",
        "    if pdm_gen == 'pwm':\n",
        "        y = pcm_to_pdm_pwm(x)\n",
        "    elif pdm_gen == 'random':\n",
        "        y = pcm_to_pdm_random(x)\n",
        "    elif pdm_gen == 'err':\n",
        "        y = pcm_to_pdm_err(x)\n",
        "    else:\n",
        "        raise ValueError('{} not known')\n",
        "    return y\n",
        "\n",
        "def pdm_to_pcm(x, n_cic):\n",
        "    x = cic1(x)\n",
        "    #x = cicn(x)\n",
        "    x = x[::ratio_out]\n",
        "    x[0] = 0\n",
        "    return x\n",
        "\n",
        "def dfe_quantized_model(x):\n",
        "    x = shift_zero_to_one(x)\n",
        "    x = x * ratio_out\n",
        "    x = x - (ratio_out / 2)\n",
        "    #x = x * 40\n",
        "    #x = x - 20\n",
        "    x = x.astype(np.int8)\n",
        "    return x\n",
        "\n",
        "def pdm_model(x_orig, model_type='fast'):\n",
        "    '''Main API interface for the pdm model.\n",
        "\n",
        "    x_orig: input 16kHz signal\n",
        "    model_type: type of PDM model\n",
        "        'fast' is fast quantization model and a worst-case distortion scenario\n",
        "        'pwm' is the medium-accuracy model which takes ~100ms per sample\n",
        "    '''\n",
        "    if model_type == 'fast':\n",
        "        return dfe_quantized_model(x_orig)\n",
        "    x_pdm = pcm_to_pdm(x_orig, pdm_gen='pwm')\n",
        "    return pdm_to_pcm(x_pdm, 1)\n",
        "\n",
        "# ==================== ACO modelling ====================\n",
        "\n",
        "maxes = {}\n",
        "def detect_max(arr, name):\n",
        "    '''Detect the maximum bit width at various stages of the pipeline. Does\n",
        "    not include the sign bit for signed numbers.'''\n",
        "    global maxes\n",
        "    absmax = np.abs(arr.astype(np.float64)).max()\n",
        "    absmax = np.max((absmax, 1))  # avoid inf in log\n",
        "    val = np.log2(absmax)  # consider absolute magnitude bits\n",
        "    # print(name, 'bitwidth', val)\n",
        "    if (name not in maxes) or (maxes[name] < val):\n",
        "        maxes[name] = val\n",
        "\n",
        "def print_maxes():\n",
        "    print('Max bit values detected during featurisation:')\n",
        "    for k in maxes:\n",
        "        print('\\t{:20} {}'.format(k, maxes[k]))\n",
        "\n",
        "def gen_dct_coefs():\n",
        "    '''Type 2 dct with 'ortho' norm.\n",
        "    See https://docs.scipy.org/doc/scipy/reference/generated/scipy.fft.dct.html.\n",
        "    '''\n",
        "    DCT_LEN = 32\n",
        "    N_COEFS = 13\n",
        "    coefs = np.zeros((N_COEFS, DCT_LEN))\n",
        "    n = np.arange(DCT_LEN)\n",
        "    for k in range(N_COEFS):\n",
        "        coefs[k,:] = np.cos(np.pi * k * (2*n + 1) / (2 * DCT_LEN))\n",
        "        if k == 0:\n",
        "            coefs[k,:] *= 2 * np.sqrt(1/(4*DCT_LEN))\n",
        "        else:\n",
        "            coefs[k,:] *= 2 * np.sqrt(1/(2*DCT_LEN))\n",
        "    coefs = np.round(coefs * 2**15).astype(np.int16)\n",
        "    return coefs.T\n",
        "\n",
        "dct_coefs = gen_dct_coefs().astype(np.int64)\n",
        "\n",
        "def aco(signal, fft_override=None):\n",
        "    '''Quantized python model of the ACO pipeline.\n",
        "\n",
        "    If fft_override is not None, use the value supplied as the fft output for\n",
        "    the rest of the calculations.'''\n",
        "    fs = 16000\n",
        "    signal = signal.astype(np.int16)  # increase bitwidth for preemphasis\n",
        "    detect_max(signal, 'signal')\n",
        "\n",
        "    # Acoustic Featurization Constants\n",
        "    FS = fs                                 # 16 KHz\n",
        "    PERIOD = 1 / FS                         # 0.0625 ms\n",
        "    SIGNAL_LENGTH = len(signal) * PERIOD    # 1 s\n",
        "    FRAME_LENGTH = .02                      # 0.02 s = 20 ms = 320 samples\n",
        "    NUM_MEL_FILTERS = 32                    # of Mel Scale filterbanks\n",
        "    NUM_CEPSTRAL = 13                       # MFCC Output\n",
        "    FFT_LENGTH = 256                        # FFT Length\n",
        "\n",
        "    # 1) preemphasis\n",
        "    # =========================================================================\n",
        "    # Delay by 1 cycle\n",
        "    rolled_signal = np.roll(signal, 1)\n",
        "    rolled_signal[0] = 0  # zero out initial value\n",
        "\n",
        "    # preemphasis coefficient of 31 / 32 = 0.96875, quantize via right shift\n",
        "    scaled_rolled_signal = np.right_shift(31 * rolled_signal, 5)\n",
        "    detect_max(scaled_rolled_signal, 'scaled_rolled_signal')\n",
        "\n",
        "    preemphasis_out = signal - scaled_rolled_signal\n",
        "    detect_max(preemphasis_out, 'preemphasis_out')\n",
        "\n",
        "    # 2) framing\n",
        "    # =========================================================================\n",
        "    # Get NUM_SAMPLES_IN_FRAME-sized non-overlapping frames, truncate as needed\n",
        "    framing_out = np.split(preemphasis_out, SIGNAL_LENGTH / FRAME_LENGTH)\n",
        "    framing_out = np.asarray(framing_out)\n",
        "    framing_out = framing_out[:, :FFT_LENGTH]\n",
        "\n",
        "    # 3) fft\n",
        "    # TODO: Check 32b quantization, scaling\n",
        "    # =========================================================================\n",
        "    # fft_out = np.fft.rfft(framing_out)\n",
        "    if fft_override is None:\n",
        "        fft_out = np.fft.rfft(framing_out) / 8  # RTL FFT scaled down by 8\n",
        "    else:\n",
        "        fft_out = fft_override\n",
        "\n",
        "    # split into real and imag\n",
        "    fft_out_real = fft_out.real\n",
        "    fft_out_imag = fft_out.imag\n",
        "    detect_max(fft_out_real, 'fft_out_real')\n",
        "    detect_max(fft_out_imag, 'fft_out_imag')\n",
        "\n",
        "    # quantize\n",
        "    fft_out_real = fft_out_real.astype(np.int32)\n",
        "    fft_out_imag = fft_out_imag.astype(np.int32)\n",
        "\n",
        "    # check 32b quantization\n",
        "    assert np.array_equal(fft_out_real, fft_out.real.astype(np.int64))\n",
        "    assert np.array_equal(fft_out_imag, fft_out.imag.astype(np.int64))\n",
        "\n",
        "    # 4) power spectrum\n",
        "    # =========================================================================\n",
        "    # power spectrum = amplitude spectrum ^ 2 = real^2 + complex^2 / FFT_PTS\n",
        "    power_spectrum_out = ((fft_out_real.astype(np.int64) *\n",
        "                           fft_out_real.astype(np.int64)) +\n",
        "                          (fft_out_imag.astype(np.int64) *\n",
        "                           fft_out_imag.astype(np.int64)))\n",
        "    detect_max(power_spectrum_out, 'power_spectrum_out')\n",
        "    # power_spectrum_out = np.right_shift(power_spectrum_out,\n",
        "                                        # int(np.log2(FFT_LENGTH)))\n",
        "\n",
        "\n",
        "    # 5) mel filterbank construction\n",
        "    # =========================================================================\n",
        "    mfcc_filterbank = speechpy.feature.filterbanks(NUM_MEL_FILTERS,\n",
        "                                                   FFT_LENGTH,\n",
        "                                                   FS)[:,:129]\n",
        "\n",
        "    # quantize\n",
        "    mfcc_filterbank = (mfcc_filterbank * (2 ** 16 - 1)).astype(np.uint16)\n",
        "\n",
        "    # 6) mel filterbank application\n",
        "    # =========================================================================\n",
        "    # shift by 16 to cancel initial quantization in step 5)\n",
        "    mfcc_out = np.dot(power_spectrum_out, mfcc_filterbank.T)\n",
        "    mfcc_out = np.right_shift(mfcc_out, 16)\n",
        "    detect_max(mfcc_out, 'mfcc_out')\n",
        "\n",
        "    # 7) log\n",
        "    # =========================================================================\n",
        "    n_frames, n_mfcc = mfcc_out.shape\n",
        "    log_out = np.zeros((n_frames, n_mfcc), dtype=np.uint8)\n",
        "    for i in range(n_frames):\n",
        "        for j in range(n_mfcc):\n",
        "            for k in range(31, -1, -1):\n",
        "                if mfcc_out[i,j] & (1 << k):\n",
        "                    log_out[i,j] = k + 1\n",
        "                    break\n",
        "    detect_max(log_out, 'log_out')\n",
        "\n",
        "    # 8) dct\n",
        "    # TODO: Check 16b quantization\n",
        "    # =========================================================================\n",
        "    # scipy.fft.dct\n",
        "    log_out = log_out.astype(np.int64)\n",
        "    dct_out_raw = np.dot(log_out, dct_coefs).astype(np.int64)\n",
        "    dct_out_raw = np.right_shift(dct_out_raw, 15)\n",
        "\n",
        "    # quantize\n",
        "    dct_out = dct_out_raw.astype(np.int16)\n",
        "    detect_max(dct_out, 'dct_out')\n",
        "\n",
        "    # check 16b quantization\n",
        "    assert np.array_equal(dct_out_raw.astype(np.int64), dct_out)\n",
        "\n",
        "    # 9) quantize to byte\n",
        "    # =========================================================================\n",
        "    quant_out = np.clip(dct_out, -2**7, 2**7-1)\n",
        "\n",
        "    # =========================================================================\n",
        "    # flatten for use in pipeline\n",
        "    out = quant_out.flatten()\n",
        "\n",
        "    # collect intermediate values for RTL verification\n",
        "    sigs = [preemphasis_out, framing_out, fft_out, power_spectrum_out,\n",
        "            mfcc_out, log_out, dct_out, quant_out, out]\n",
        "\n",
        "    return sigs\n",
        "\n",
        "def aco_with_dfe(fullfname):\n",
        "    '''Quantized python model of the ACO pipeline.'''\n",
        "    # Get raw 16b audio data\n",
        "    fs, signal = wavfile.read(fullfname)\n",
        "\n",
        "    # DFE quantization model\n",
        "    signal = pdm_model(signal, model_type='fast')\n",
        "    #print(signal.shape)\n",
        "    #signal = pdm_model(signal, model_type='accurate')\n",
        "    #print(signal.shape)\n",
        "    return aco(signal)[-1]\n",
        "\n",
        "def get_speechpy_features(fullfname):\n",
        "    '''Reads a .wav file and outputs the MFCC features using SpeechPy.'''\n",
        "    # parameters:\n",
        "    p = {'num_mfcc_cof': 13,\n",
        "         'frame_length': 0.02,\n",
        "         'frame_stride': 0.02,\n",
        "         'filter_num': 32,\n",
        "         'fft_length': 256,\n",
        "         'window_size': 101,\n",
        "         'low_frequency': 300,\n",
        "         'preemph_cof': 0.98}\n",
        "    try:\n",
        "        fs, data = wavfile.read(fullfname)\n",
        "    except ValueError:\n",
        "        print('failed to read file {}, continuing'.format(fullfname))\n",
        "        return np.zeros(650)\n",
        "\n",
        "    # generate features\n",
        "    preemphasized = speechpy.processing.preemphasis(data, cof=p['preemph_cof'], shift=1)\n",
        "    mfcc = speechpy.feature.mfcc(preemphasized, fs, frame_length=p['frame_length'],\n",
        "                                  frame_stride=p['frame_stride'], num_cepstral=p['num_mfcc_cof'],\n",
        "                                  num_filters=p['filter_num'], fft_length=p['fft_length'],\n",
        "                                  low_frequency=p['low_frequency'])\n",
        "    #print('mfcc shape', mfcc.shape)\n",
        "    # TODO: Why is the output shape here (49, 13) and not (50, 13)?\n",
        "    # For now just repeat last frame:\n",
        "    mfcc2 = np.zeros((50, 13))\n",
        "    mfcc2[:-1,:] = mfcc\n",
        "    mfcc2[-1,:] = mfcc[-1,:]\n",
        "\n",
        "    mfcc_cmvn = speechpy.processing.cmvnw(mfcc2, win_size=p['window_size'], variance_normalization=True)\n",
        "\n",
        "    flattened = mfcc_cmvn.flatten()\n",
        "    return flattened\n",
        "\n",
        "def get_fnames_group(group):\n",
        "    '''Gets all the filenames (with directories) for a given class.'''\n",
        "    dir = group + '/'\n",
        "    fnames = os.listdir(dir)\n",
        "    fnames = [x for x in fnames if not x.startswith('.')]  # ignore .DS_Store\n",
        "    fullnames = [dir + fname for fname in fnames]\n",
        "    return fullnames\n",
        "\n",
        "def get_fnames_and_labels():\n",
        "    '''Collect a big list of filenames and a big list of labels.'''\n",
        "    all_fnames = []\n",
        "    all_labels = []\n",
        "    for group in ['yes', 'unknown', 'noise']:\n",
        "        fnames = get_fnames_group(group)\n",
        "        label = 1 if group == 'yes' else 2\n",
        "        repeat = 2 if group == 'yes' else 1  # oversample wake word class to balance dataset\n",
        "        for _ in range(repeat):\n",
        "            all_fnames.extend(fnames)\n",
        "            for i in range(len(fnames)):\n",
        "                all_labels.append(label)\n",
        "    all_labels = np.array(all_labels)\n",
        "    return all_fnames, all_labels\n",
        "\n",
        "def get_features_quantized(all_fnames):\n",
        "    '''Get a big list of mfcc features for each wav file.'''\n",
        "    n = len(all_fnames)\n",
        "    # n = 1\n",
        "    print('num samples: ', n)\n",
        "    all_features = np.zeros((0, 13*50))\n",
        "    for i in tqdm(range(n)):\n",
        "        fname = all_fnames[i]\n",
        "        features = aco_with_dfe(fname)\n",
        "        all_features = np.vstack((all_features, features))\n",
        "    print_maxes()\n",
        "    return all_features\n",
        "\n",
        "def shuffle_and_split(all_features, all_labels):\n",
        "    '''First shuffle the data randomly, then split it into test and train.'''\n",
        "    np.random.seed(1)\n",
        "    n = len(all_labels)\n",
        "    idx = np.arange(n, dtype=int)\n",
        "    np.random.shuffle(idx)\n",
        "    features_shuffled = all_features[idx,:]\n",
        "    labels_shuffled = all_labels[idx]\n",
        "\n",
        "    # split the data into train and test sets\n",
        "    split = int(train_test_split * n)\n",
        "    X = features_shuffled[:split,:]\n",
        "    Y = labels_shuffled[:split]\n",
        "    Xtest = features_shuffled[split:,:]\n",
        "    Ytest = labels_shuffled[split:]\n",
        "    return X, Y, Xtest, Ytest\n",
        "\n",
        "def generate_features():\n",
        "    '''Main function for generating MFCC features.'''\n",
        "    all_fnames, all_labels = get_fnames_and_labels()\n",
        "    all_features = get_features_quantized(all_fnames)\n",
        "    return shuffle_and_split(all_features, all_labels)\n",
        "\n",
        "X, Y, Xtest, Ytest = generate_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting speechpy\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/12/dbda397a998063d9541d9e149c4f523ed138a48824d20598e37632ba33b1/speechpy-2.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from speechpy) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from speechpy) (1.19.5)\n",
            "Installing collected packages: speechpy\n",
            "Successfully installed speechpy-2.4\n",
            "num samples:  6036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c98c548bf0b244a19bea2ce61d421bea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=6036.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Max bit values detected during featurisation:\n",
            "\tsignal               6.965784284662087\n",
            "\tscaled_rolled_signal 6.930737337562887\n",
            "\tpreemphasis_out      7.94251450533924\n",
            "\tfft_out_real         10.413798263822736\n",
            "\tfft_out_imag         10.289454809428715\n",
            "\tpower_spectrum_out   20.97661796057677\n",
            "\tmfcc_out             22.303562823169067\n",
            "\tlog_out              4.523561956057013\n",
            "\tdct_out              6.475733430966398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "OPi-1yClDQAY",
        "outputId": "9319eec8-36a6-4537-d127-b7c8f2f166a7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot the dataset balance\n",
        "\n",
        "def plot_dist(labels, title):\n",
        "    plt.figure()\n",
        "    nl = len(np.unique(labels))\n",
        "    freqs = np.zeros(nl)\n",
        "    for label in Y:\n",
        "        freqs[label-1] += 1\n",
        "    plt.bar(np.arange(nl)+1, freqs)\n",
        "    plt.title(title)\n",
        "\n",
        "plot_dist(Y, 'Training dataset balance')\n",
        "plot_dist(Ytest, 'Testing dataset balance')\n",
        "\n",
        "yes_features = X[Y==1,:]\n",
        "unknown_features = X[Y!=1,:]\n",
        "\n",
        "def plot_samples():\n",
        "    # plot some samples\n",
        "    for i in range(5):\n",
        "        plt.figure()\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(yes_features[i,:].reshape(50, 13).T)\n",
        "        plt.title('Yes sample')\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(unknown_features[i,:].reshape(50, 13).T)\n",
        "        plt.title('Unknown sample')\n",
        "  \n",
        "def plot_average_sample():\n",
        "    # plot the average yes and unknown sample\n",
        "    avg_yes = np.zeros(50, 13)\n",
        "    avg_unknown = np.zeros(50, 13)\n",
        "    for i in range(len(yes_features)):\n",
        "        avg_yes += yes_features[i,:].reshape(50, 13)\n",
        "    for i in range(len(unknown_features)):\n",
        "        avg_yes += unknown_features[i,:].reshape(50, 13)\n",
        "    avg_yes / len(yes_features)\n",
        "    avg_unknown / len(unknown_features)\n",
        "    plt.figure()\n",
        "    plt.imshow(avg_yes.T)\n",
        "    plt.title('average yes')\n",
        "    plt.figure()\n",
        "    plt.imshow(avg_unknown.T)\n",
        "    plt.title('average unknown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWPRZ1QgqKMm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbiiMcdNJI--"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3RF5VmcoUMG"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(13, 8, 3, padding=1, bias=False)\n",
        "        self.pool = nn.MaxPool1d(2, 2, ceil_mode=True)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.conv2 = nn.Conv1d(8, 16, 3, padding=1, bias=False)\n",
        "        self.fc1 = nn.Linear(16 * 13, 2, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.reshape((x.shape[0], 50, 13)).transpose(1, 2)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.reshape((x.shape[0], 16*13))\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 500\n",
        "\n",
        "Y_tensor = torch.Tensor(Y-1).type(torch.int64)\n",
        "trainset = TensorDataset(torch.Tensor(X), Y_tensor)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size)\n",
        "\n",
        "Ytest_tensor = torch.Tensor(Ytest-1).type(torch.int64)\n",
        "testset = TensorDataset(torch.Tensor(Xtest), Ytest_tensor)\n",
        "testloader = DataLoader(testset, batch_size=batch_size)\n",
        "\n",
        "def train(model: nn.Module, dataloader: DataLoader):\n",
        "    model.train()  # turn on dropout layers\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        #for i in range(len(X)):\n",
        "        #    inputs = torch.tensor(X[i,:]).reshape((1, 650))\n",
        "        #    labels = torch.tensor(Y[i] - 1, dtype=torch.int64).reshape(1)\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        if epoch % 10 == 9:\n",
        "            print('[%d] loss: %.3f' % (epoch + 1, running_loss))\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n",
        "    model.eval()  # turn off dropout layers\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    n_inferences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #for i in range(len(X)):\n",
        "        #    inputs = torch.tensor(X[i,:]).reshape((1, 650))\n",
        "        #    labels = torch.tensor(Y[i]).reshape(1)\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if max_samples:\n",
        "                n_inferences += inputs.shape[0]\n",
        "                if n_inferences > max_samples:\n",
        "                    break\n",
        "    \n",
        "    return 100 * correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HixhBHaqtmZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d0e190-788f-4224-fc9f-abbcd3dcd1aa"
      },
      "source": [
        "train(net, trainloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10] loss: 36.639\n",
            "[20] loss: 25.661\n",
            "[30] loss: 20.705\n",
            "[40] loss: 19.027\n",
            "[50] loss: 17.947\n",
            "[60] loss: 16.273\n",
            "[70] loss: 16.609\n",
            "[80] loss: 16.153\n",
            "[90] loss: 14.772\n",
            "[100] loss: 14.412\n",
            "[110] loss: 15.826\n",
            "[120] loss: 13.889\n",
            "[130] loss: 14.296\n",
            "[140] loss: 13.814\n",
            "[150] loss: 13.375\n",
            "[160] loss: 13.081\n",
            "[170] loss: 14.026\n",
            "[180] loss: 13.442\n",
            "[190] loss: 13.630\n",
            "[200] loss: 12.766\n",
            "[210] loss: 12.807\n",
            "[220] loss: 12.102\n",
            "[230] loss: 12.885\n",
            "[240] loss: 12.802\n",
            "[250] loss: 13.366\n",
            "[260] loss: 13.208\n",
            "[270] loss: 13.054\n",
            "[280] loss: 12.392\n",
            "[290] loss: 13.162\n",
            "[300] loss: 12.065\n",
            "[310] loss: 11.970\n",
            "[320] loss: 12.921\n",
            "[330] loss: 12.298\n",
            "[340] loss: 13.016\n",
            "[350] loss: 11.462\n",
            "[360] loss: 11.093\n",
            "[370] loss: 11.657\n",
            "[380] loss: 13.664\n",
            "[390] loss: 12.802\n",
            "[400] loss: 10.986\n",
            "[410] loss: 11.115\n",
            "[420] loss: 12.857\n",
            "[430] loss: 12.180\n",
            "[440] loss: 11.684\n",
            "[450] loss: 12.502\n",
            "[460] loss: 11.165\n",
            "[470] loss: 11.583\n",
            "[480] loss: 12.238\n",
            "[490] loss: 11.261\n",
            "[500] loss: 11.497\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27_n-djuEdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d19187-d5e3-46f5-a2fa-409cb5cae2e5"
      },
      "source": [
        "score = test(net, trainloader)\n",
        "print('Accuracy of the network on the train data: {}%'.format(score))\n",
        "score = test(net, testloader)\n",
        "print('Accuracy of the network on the test data: {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the train data: 98.93969516235917%\n",
            "Accuracy of the network on the test data: 97.2829688535454%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7l7yQEtNxh5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZoEjBSveV8"
      },
      "source": [
        "# Part 1: Visualize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVWbC5YWT-MU"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# A convenience function which we use to copy CNNs\n",
        "def copy_model(model: nn.Module) -> nn.Module:\n",
        "    result = deepcopy(model)\n",
        "\n",
        "    # Copy over the extra metadata we've collected which copy.deepcopy doesn't capture\n",
        "    if hasattr(model, 'input_activations'):\n",
        "        result.input_activations = deepcopy(model.input_activations)\n",
        "\n",
        "    for result_layer, original_layer in zip(result.children(), model.children()):\n",
        "        if isinstance(result_layer, nn.Conv1d) or isinstance(result_layer, nn.Linear):\n",
        "            if hasattr(original_layer.weight, 'scale'):\n",
        "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
        "            if hasattr(original_layer, 'activations'):\n",
        "                result_layer.activations = deepcopy(original_layer.activations)\n",
        "            if hasattr(original_layer, 'output_scale'):\n",
        "                result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qKRX7ply7I2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2h7zJ8m3GAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "outputId": "1e197d39-6faf-4269-91f9-642b5e599c07"
      },
      "source": [
        "# ADD YOUR CODE HERE to plot distributions of weights\n",
        "\n",
        "# You can get a flattened vector of the weights of fc1 like this:\n",
        "#   fc1_weights = net.fc1.weight.data.cpu().view(-1)\n",
        "# Try plotting a histogram of fc1_weights (and the weights of all the other layers as well)\n",
        "\n",
        "def show_weight_dist(layer, title):\n",
        "  weights = layer.weight.data.cpu().view(-1)\n",
        "  plt.figure()\n",
        "  plt.hist(weights, 100)\n",
        "  plt.title(title)\n",
        "  min = weights.min()\n",
        "  max = weights.max()\n",
        "  mean = weights.mean()\n",
        "  std = weights.std()\n",
        "  plt.axvline(mean - 3*std)\n",
        "  plt.axvline(mean + 3*std)\n",
        "  print('{} min {:.03f}, max {:.03f}, mean'.format(title, min, max) +\n",
        "         ' {:.03f}, 3-sigma ({:.03f}, {:.03f})'.format(mean, mean - 3*std, mean + 3*std) +\n",
        "         ', max-min {:.03f}, 3-sigma range {:.03f}'.format(max - min, 6*std))\n",
        "\n",
        "show_weight_dist(net.conv1, 'conv1')\n",
        "show_weight_dist(net.conv2, 'conv2')\n",
        "show_weight_dist(net.fc1, 'fc1')\n",
        "\n",
        "# fc3 has a 3-sigma range that is greater than the max-min, would want to use\n",
        "# max-min as the range and not the 3-sigma one"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 min -0.832, max 0.702, mean -0.010, 3-sigma (-0.858, 0.838), max-min 1.535, 3-sigma range 1.696\n",
            "conv2 min -0.547, max 0.534, mean -0.054, 3-sigma (-0.588, 0.479), max-min 1.081, 3-sigma range 1.067\n",
            "fc1 min -0.349, max 0.462, mean -0.000, 3-sigma (-0.399, 0.398), max-min 0.811, 3-sigma range 0.797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKjshaHD11m"
      },
      "source": [
        "# Part 2: Quantize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXNk1fXuPGjB"
      },
      "source": [
        "net_q2 = copy_model(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIBrqSFrVi5x"
      },
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def quantized_weights(weights: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
        "    '''\n",
        "    Quantize the weights so that all values are integers between -128 and 127.\n",
        "    You may want to use the total range, 3-sigma range, or some other range when\n",
        "    deciding just what factors to scale the float32 values by.\n",
        "\n",
        "    Parameters:\n",
        "    weights (Tensor): The unquantized weights\n",
        "\n",
        "    Returns:\n",
        "    (Tensor, float): A tuple with the following elements:\n",
        "                        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
        "                          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "                        * The scaling factor that your weights were multiplied by.\n",
        "                          This value does not need to be an 8-bit integer.\n",
        "    '''\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "    # std = weights.std()\n",
        "    absmax = weights.abs().max()\n",
        "    # scale = 127 / (3 * std)  # 3-sigma\n",
        "    scale = 127 / absmax\n",
        "    result = (weights * scale).round()\n",
        "    #return torch.clamp(result, min=-128, max=127), scale\n",
        "    return result, scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orOwTnXxU1nb"
      },
      "source": [
        "def quantize_layer_weights(model: nn.Module):\n",
        "    for layer in model.children():\n",
        "        if isinstance(layer, nn.Conv1d) or isinstance(layer, nn.Linear):\n",
        "            q_layer_data, scale = quantized_weights(layer.weight.cpu().data)\n",
        "            q_layer_data = q_layer_data.to(device)\n",
        "\n",
        "            layer.weight.data = q_layer_data\n",
        "            layer.weight.scale = scale\n",
        "\n",
        "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
        "            if (q_layer_data != q_layer_data.round()).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
        "\n",
        "quantize_layer_weights(net_q2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE3HqeBKVoYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417f6221-caeb-4a2c-e936-ab02f87f38f5"
      },
      "source": [
        "score = test(net_q2, trainloader)\n",
        "print('Accuracy of the network after quantizing all weights (train): {}%'.format(score))\n",
        "score = test(net_q2, testloader)\n",
        "print('Accuracy of the network after quantizing all weights (test): {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network after quantizing all weights (train): 99.00596421471172%\n",
            "Accuracy of the network after quantizing all weights (test): 96.48774022531478%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsXmu8AZg41F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea72609-30aa-452d-fc49-339a1ae06a95"
      },
      "source": [
        "net_q2.conv1.weight.data.cpu().numpy().min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-127.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7bfTF1bBVe"
      },
      "source": [
        "# Part 3: Visualize Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP587b0QYxe9"
      },
      "source": [
        "def register_activation_profiling_hooks(model: Net):\n",
        "    model.input_activations = np.empty(0)\n",
        "    model.conv1.activations = np.empty(0)\n",
        "    model.conv2.activations = np.empty(0)\n",
        "    model.fc1.activations = np.empty(0)\n",
        "\n",
        "    model.profile_activations = True\n",
        "\n",
        "    def conv1_activations_hook(layer, x, y):\n",
        "        if model.profile_activations:\n",
        "            model.input_activations = np.append(model.input_activations, x[0].cpu().flatten())\n",
        "    model.conv1.register_forward_hook(conv1_activations_hook)\n",
        "\n",
        "    def conv2_activations_hook(layer, x, y):\n",
        "        if model.profile_activations:\n",
        "            model.conv1.activations = np.append(model.conv1.activations, x[0].cpu().flatten())\n",
        "    model.conv2.register_forward_hook(conv2_activations_hook)\n",
        "\n",
        "    def fc1_activations_hook(layer, x, y):\n",
        "        if model.profile_activations:\n",
        "            model.conv2.activations = np.append(model.conv2.activations, x[0].cpu().flatten())\n",
        "            model.fc1.activations = np.append(model.fc1.activations, y[0].cpu().flatten())\n",
        "    model.fc1.register_forward_hook(fc1_activations_hook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVvPCIoabLC7"
      },
      "source": [
        "net_q3 = copy_model(net)\n",
        "register_activation_profiling_hooks(net_q3)\n",
        "\n",
        "# Run through the training dataset again while profiling the input and output activations this time\n",
        "# We don't actually have to perform gradient descent for this, so we can use the \"test\" function\n",
        "test(net_q3, trainloader, max_samples=400)\n",
        "net_q3.profile_activations = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1HnYsuAMoxP"
      },
      "source": [
        "input_activations = net_q3.input_activations\n",
        "conv1_output_activations = net_q3.conv1.activations\n",
        "conv2_output_activations = net_q3.conv2.activations\n",
        "fc1_output_activations = net_q3.fc1.activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEo8VK46bwjn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e840248-d5d2-439e-8229-b0c998a7ca93"
      },
      "source": [
        "# ADD YOUR CODE HERE to plot distributions of activations\n",
        "\n",
        "def show_weight_dist(data, title):\n",
        "    plt.figure()\n",
        "    plt.hist(data, 100)\n",
        "    plt.title(title)\n",
        "    min = data.min()\n",
        "    max = data.max()\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "    #plt.axvline(mean - 3*std)\n",
        "    plt.axvline(mean + 3*std)\n",
        "    print('{} min {:.03f}, max {:.03f}, mean'.format(title, min, max) +\n",
        "          ' {:.03f}, 3-sigma ({:.03f}, {:.03f})'.format(mean, mean - 3*std, mean + 3*std) +\n",
        "          ', max-min {:.03f}, 3-sigma range {:.03f}'.format(max - min, 6*std))\n",
        "\n",
        "show_weight_dist(input_activations, 'input')\n",
        "show_weight_dist(conv1_output_activations, 'conv1')\n",
        "show_weight_dist(conv2_output_activations, 'conv2')\n",
        "show_weight_dist(fc1_output_activations, 'fc1')\n",
        "\n",
        "# Plot histograms of the following variables, and calculate their ranges and 3-sigma ranges:\n",
        "#   input_activations\n",
        "#   conv1_output_activations\n",
        "#   conv2_output_activations\n",
        "#   fc1_output_activations\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input min -37.000, max 84.000, mean 0.581, 3-sigma (-18.029, 19.191), max-min 121.000, 3-sigma range 37.220\n",
            "conv1 min 0.000, max 33.561, mean 0.807, 3-sigma (-6.368, 7.982), max-min 33.561, 3-sigma range 14.350\n",
            "conv2 min 0.000, max 11.266, mean 0.286, 3-sigma (-2.155, 2.726), max-min 11.266, 3-sigma range 4.881\n",
            "fc1 min -9.988, max 10.468, mean -0.021, 3-sigma (-13.897, 13.855), max-min 20.456, 3-sigma range 27.752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9PUHh4NjKc5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haiPVx4ibEra"
      },
      "source": [
        "# Part 4: Quantize Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLjSp7hsXofq"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "class NetQuantized(nn.Module):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantized, self).__init__()\n",
        "        \n",
        "        net_init = copy_model(net_with_weights_quantized)\n",
        "\n",
        "        self.conv1 = net_init.conv1\n",
        "        self.pool = net_init.pool\n",
        "        self.conv2 = net_init.conv2\n",
        "        self.fc1 = net_init.fc1\n",
        "\n",
        "        for layer in self.conv1, self.conv2, self.fc1:\n",
        "            def pre_hook(l, x):\n",
        "                x = x[0]\n",
        "                if (x < -128).any() or (x > 127).any():\n",
        "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
        "                if (x != x.round()).any():\n",
        "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
        "\n",
        "            layer.register_forward_pre_hook(pre_hook)\n",
        "\n",
        "        # Calculate the scaling factor for the initial input to the CNN\n",
        "        self.input_activations = net_with_weights_quantized.input_activations\n",
        "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
        "\n",
        "        # Calculate the output scaling factors for all the layers of the CNN\n",
        "        preceding_layer_scales = []\n",
        "        for layer in self.conv1, self.conv2, self.fc1:\n",
        "            layer.output_scale = NetQuantized.quantize_activations(layer.activations, layer.weight.scale, self.input_scale, preceding_layer_scales)\n",
        "            preceding_layer_scales.append((layer.weight.scale, layer.output_scale))\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_initial_input(pixels: np.ndarray) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
        "\n",
        "        Parameters:\n",
        "        pixels (ndarray): The values of all the pixels which were part of the input image during training\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "        #absmax = np.abs(pixels).max()  # uncomment to quantized activations to int8 range\n",
        "        #return 127 / absmax\n",
        "        return 1  # initial input activations not quantized in wakey_wakey\n",
        "\n",
        "    @staticmethod\n",
        "    def quantize_activations(activations: np.ndarray, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]]) -> float:\n",
        "        '''\n",
        "        Calculate a scaling factor to multiply the output of a layer by.\n",
        "\n",
        "        Parameters:\n",
        "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        float: A scaling factor that the layer output should be multiplied by before being fed into the first layer.\n",
        "               This value does not need to be an 8-bit integer.\n",
        "        '''\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "        max = activations.max()\n",
        "        product = 1\n",
        "        for prev_scaling in ns:\n",
        "            product *= prev_scaling[0] * prev_scaling[1]\n",
        "        scale = 127 / (n_w * product * n_initial_input * max)  # return this for original output\n",
        "        return 2 ** np.floor(np.log2(scale))  # make scaling a power of 2!\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # You can access the output activation scales like this:\n",
        "        #   fc1_output_scale = self.fc1.output_scale\n",
        "\n",
        "        # To make sure that the outputs of each layer are integers between -128 and 127, you may need to use the following functions:\n",
        "        #   * torch.Tensor.round\n",
        "        #   * torch.clamp\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "        x = x.reshape((x.shape[0], 50, 13)).transpose(1, 2)\n",
        "        x = torch.clamp(torch.round(x * self.input_scale), min=-128, max=127)\n",
        "        x = self.pool(torch.clamp(torch.floor(F.relu(self.conv1(x)) * self.conv1.output_scale), min=-128, max=127))\n",
        "        x = self.pool(torch.clamp(torch.floor(F.relu(self.conv2(x)) * self.conv2.output_scale), min=-128, max=127))\n",
        "        x = x.reshape((x.shape[0], 16*13))\n",
        "        #x = torch.clamp(torch.floor(self.fc1(x) * self.fc1.output_scale), min=-128, max=127)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13CpHgvE994J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d20700-9241-4175-fcc4-ee77959bd108"
      },
      "source": [
        "# Merge the information from net_q2 and net_q3 together\n",
        "net_init = copy_model(net_q2)\n",
        "net_init.input_activations = deepcopy(net_q3.input_activations)\n",
        "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
        "    if isinstance(layer_init, nn.Conv1d) or isinstance(layer_init, nn.Linear):\n",
        "        layer_init.activations = deepcopy(layer_q3.activations)\n",
        "\n",
        "net_quantized = NetQuantized(net_init)\n",
        "\n",
        "for l in [net_quantized.conv1, net_quantized.conv2, net_quantized.fc1]:\n",
        "    print(np.log2(l.output_scale))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-6.)\n",
            "tensor(-6.)\n",
            "tensor(-8.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur6Rckyjy7tS",
        "outputId": "853ae971-ad69-46bd-8cd0-a61afd1558ed"
      },
      "source": [
        "score = test(net_quantized, trainloader)\n",
        "print('Accuracy of the network after quantizing activations (train): {}%'.format(score))\n",
        "score = test(net_quantized, testloader)\n",
        "print('Accuracy of the network after quantizing activations (test): {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network after quantizing activations (train): 98.93969516235917%\n",
            "Accuracy of the network after quantizing activations (test): 96.55400927766733%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpnyPRxzyNc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jTOL7scbMs7"
      },
      "source": [
        "# Part 5: Quantize Biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JI3f00ZdVKM"
      },
      "source": [
        "class NetWithBias(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWithBias, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(13, 8, 3, padding=1, bias=True)\n",
        "        self.pool = nn.MaxPool1d(2, 2, ceil_mode=True)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.conv2 = nn.Conv1d(8, 16, 3, padding=1, bias=True)\n",
        "        self.fc1 = nn.Linear(16 * 13, 2, bias=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.reshape((x.shape[0], 50, 13)).transpose(1, 2)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.reshape((x.shape[0], 16*13))\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "net_with_bias = NetWithBias().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjk3hEQaVDpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f3f5f2-0001-4120-a436-8ced074b87cf"
      },
      "source": [
        "train(net_with_bias, trainloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10] loss: 31.146\n",
            "[20] loss: 24.718\n",
            "[30] loss: 21.777\n",
            "[40] loss: 19.631\n",
            "[50] loss: 18.032\n",
            "[60] loss: 17.671\n",
            "[70] loss: 16.714\n",
            "[80] loss: 15.369\n",
            "[90] loss: 13.943\n",
            "[100] loss: 14.361\n",
            "[110] loss: 12.655\n",
            "[120] loss: 12.969\n",
            "[130] loss: 12.416\n",
            "[140] loss: 13.746\n",
            "[150] loss: 13.104\n",
            "[160] loss: 13.255\n",
            "[170] loss: 12.441\n",
            "[180] loss: 14.009\n",
            "[190] loss: 13.119\n",
            "[200] loss: 11.678\n",
            "[210] loss: 13.460\n",
            "[220] loss: 12.735\n",
            "[230] loss: 12.466\n",
            "[240] loss: 11.340\n",
            "[250] loss: 12.178\n",
            "[260] loss: 11.508\n",
            "[270] loss: 11.204\n",
            "[280] loss: 12.102\n",
            "[290] loss: 11.855\n",
            "[300] loss: 11.579\n",
            "[310] loss: 12.416\n",
            "[320] loss: 11.213\n",
            "[330] loss: 11.456\n",
            "[340] loss: 11.822\n",
            "[350] loss: 12.151\n",
            "[360] loss: 11.132\n",
            "[370] loss: 10.956\n",
            "[380] loss: 10.383\n",
            "[390] loss: 10.911\n",
            "[400] loss: 10.191\n",
            "[410] loss: 10.330\n",
            "[420] loss: 10.818\n",
            "[430] loss: 10.554\n",
            "[440] loss: 11.273\n",
            "[450] loss: 9.565\n",
            "[460] loss: 11.271\n",
            "[470] loss: 9.742\n",
            "[480] loss: 10.407\n",
            "[490] loss: 10.561\n",
            "[500] loss: 10.232\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9ohsZVmdpGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b746e670-d080-4e17-b7f1-d6c14e212169"
      },
      "source": [
        "score = test(net_with_bias, trainloader)\n",
        "print('Accuracy of the network with bias (train): {}%'.format(score))\n",
        "score = test(net_with_bias, testloader)\n",
        "print('Accuracy of the network with bias (test): {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network with bias (train): 98.76297768941905%\n",
            "Accuracy of the network with bias (test): 96.35520212060968%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ZiJk6yEEM-"
      },
      "source": [
        "register_activation_profiling_hooks(net_with_bias)\n",
        "test(net_with_bias, trainloader, max_samples=400)\n",
        "net_with_bias.profile_activations = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZwk8KLtAUAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace82fe7-af2d-4e02-82a9-2e79e8b1185d"
      },
      "source": [
        "net_with_bias_with_quantized_weights = copy_model(net_with_bias)\n",
        "quantize_layer_weights(net_with_bias_with_quantized_weights)\n",
        "\n",
        "score = test(net_with_bias_with_quantized_weights, trainloader)\n",
        "print('Accuracy of the network on the train images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))\n",
        "score = test(net_with_bias_with_quantized_weights, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the train images after all the weights are quantized but the bias isn't: 97.6805831676607%\n",
            "Accuracy of the network on the test images after all the weights are quantized but the bias isn't: 95.75878064943672%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO2Gdu_tEZ4v"
      },
      "source": [
        "class NetQuantizedWithBias(NetQuantized):\n",
        "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
        "        super(NetQuantizedWithBias, self).__init__(net_with_weights_quantized)\n",
        "\n",
        "\n",
        "        for i, l in enumerate([self.conv1, self.conv2, self.fc1]):\n",
        "            preceding_scales = [(layer.weight.scale, layer.output_scale) for layer in self.children() if isinstance(layer, nn.Conv1d) or isinstance(layer, nn.Linear)][:i]\n",
        "\n",
        "            l.bias.data = NetQuantizedWithBias.quantized_bias(\n",
        "                l.bias.data,\n",
        "                l.weight.scale,\n",
        "                self.input_scale,\n",
        "                preceding_scales\n",
        "            )\n",
        "\n",
        "            if (l.bias.data < -2147483648).any() or (l.bias.data > 2147483647).any():\n",
        "                raise Exception(\"Bias has values which are out of bounds for an 32-bit signed integer\")\n",
        "            if (l.bias.data != l.bias.data.round()).any():\n",
        "                raise Exception(\"Bias has non-integer values\")\n",
        "\n",
        "    @staticmethod\n",
        "    def quantized_bias(bias: torch.Tensor, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]]) -> torch.Tensor:\n",
        "        '''\n",
        "        Quantize the bias so that all values are integers between -2147483648 and 2147483647.\n",
        "\n",
        "        Parameters:\n",
        "        bias (Tensor): The floating point values of the bias\n",
        "        n_w (float): The scale by which the weights of this layer were multiplied\n",
        "        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
        "        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
        "\n",
        "        Returns:\n",
        "        Tensor: The bias in quantized form, where every value is an integer between -2147483648 and 2147483647.\n",
        "                The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
        "        '''\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "        product = 1\n",
        "        for prev_scaling in ns:\n",
        "            product *= prev_scaling[0] * prev_scaling[1]\n",
        "        scale = (n_w * product * n_initial_input)\n",
        "        return (bias * scale).round()\n",
        "        #return torch.clamp((bias * 2.5).round(), min=-2147483648, max=2147483647)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA6rXt3Q-zF8"
      },
      "source": [
        "net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ho_iCwfVoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6b4a17-7db5-4f63-e0fd-8dc677e944f4"
      },
      "source": [
        "score = test(net_quantized_with_bias, trainloader)\n",
        "print('Accuracy of the network with quantized weights and bias (train): {}%'.format(score))\n",
        "score = test(net_quantized_with_bias, testloader)\n",
        "print('Accuracy of the network with quantized weights and bias (test): {}%'.format(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network with quantized weights and bias (train): 98.80715705765408%\n",
            "Accuracy of the network with quantized weights and bias (test): 96.55400927766733%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ga8xxxLhrc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b6d6986-4db6-4eac-f8c2-568cc8c6fe3a"
      },
      "source": [
        "print('Number of parameters in each layer:')\n",
        "for p in net_quantized_with_bias.parameters():\n",
        "    print(p.numel())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters in each layer:\n",
            "312\n",
            "8\n",
            "384\n",
            "16\n",
            "416\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDriUNRThyQF"
      },
      "source": [
        "# Part 6: Numpy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgbtLHSRh3Pd"
      },
      "source": [
        "conv1_weights = net_quantized_with_bias.conv1.weight.data.cpu().numpy().astype(np.int8)\n",
        "conv1_biases = net_quantized_with_bias.conv1.bias.data.cpu().numpy().astype(np.int32)\n",
        "conv2_weights = net_quantized_with_bias.conv2.weight.data.cpu().numpy().astype(np.int8)\n",
        "conv2_biases = net_quantized_with_bias.conv2.bias.data.cpu().numpy().astype(np.int32)\n",
        "fc1_weights = net_quantized_with_bias.fc1.weight.data.cpu().numpy().astype(np.int8)\n",
        "fc1_biases = net_quantized_with_bias.fc1.bias.data.cpu().numpy().astype(np.int32)\n",
        "\n",
        "# transpose the weights so that they match the numpy model\n",
        "conv1_weights = np.transpose(conv1_weights, (2, 1, 0))\n",
        "conv2_weights = np.transpose(conv2_weights, (2, 1, 0))\n",
        "#fc1_weights = np.transpose(fc1_weights, (1, 0))  # works if x is transposed\n",
        "fc1_weights = np.transpose(fc1_weights.reshape(2, 16, 13), (0, 2, 1)).reshape(2, 208).T\n",
        "\n",
        "# get the featuremap scalings\n",
        "input_scale = net_quantized_with_bias.input_scale\n",
        "conv1_output_scale = net_quantized_with_bias.conv1.output_scale\n",
        "conv2_output_scale = net_quantized_with_bias.conv2.output_scale\n",
        "fc1_output_scale = net_quantized_with_bias.fc1.output_scale\n",
        "\n",
        "# bitshifts is the amount that each featuremap is shifted to the right eg: 8 would be a divide by 256\n",
        "bitshifts = -np.log2(np.array([conv1_output_scale, conv2_output_scale, fc1_output_scale])).astype(np.int8)\n",
        "\n",
        "input_scale_arr = np.array(input_scale)  # single element array for input featuremap scaling\n",
        "\n",
        "weights_list = [input_scale_arr, bitshifts,\n",
        "                conv1_weights, conv1_biases,\n",
        "                conv2_weights, conv2_biases,\n",
        "                fc1_weights, fc1_biases]\n",
        "np.savez('parameters_quantized.npz', *weights_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIeMZLeW7Fer",
        "outputId": "2e3bc579-b143-4ea2-86f9-95f64a383aa4"
      },
      "source": [
        "weights_archive = np.load('parameters_quantized.npz')\n",
        "weights_list = [weights_archive['arr_{}'.format(i)] for i in range(len(weights_archive))]\n",
        "\n",
        "for weights in weights_list:\n",
        "    print(weights.shape)\n",
        "\n",
        "input_scale = weights_list[0]\n",
        "bitshifts = weights_list[1]\n",
        "conv1_weights = weights_list[2]\n",
        "conv1_biases = weights_list[3]\n",
        "conv2_weights = weights_list[4]\n",
        "conv2_biases = weights_list[5]\n",
        "fc1_weights = weights_list[6]\n",
        "fc1_biases = weights_list[7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "()\n",
            "(3,)\n",
            "(3, 13, 8)\n",
            "(8,)\n",
            "(3, 8, 16)\n",
            "(16,)\n",
            "(208, 2)\n",
            "(2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyl-WV3OiRpP"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "def conv1d_single_kernel(x, weights, bias):\n",
        "    '''Perform convolution of a input feature map with a single filter and bias.\n",
        "    \n",
        "    featuremap dims are (time, n_coeffs), so (50, 13)\n",
        "    '''\n",
        "    out = np.zeros(x.shape[0], dtype=np.int64)  # output length is the same\n",
        "    x_pad = np.zeros((x.shape[0]+2, x.shape[1]), dtype=np.int64)\n",
        "    x_pad[1:-1,:] = x  # zero padding\n",
        "    for i in range(x.shape[0]):\n",
        "        section = x_pad[i:i+3,:]\n",
        "        out[i] = relu(np.sum(section * weights) + bias)\n",
        "    return out\n",
        "\n",
        "def conv1d_multi_kernel(x, weights, biases):\n",
        "    n_kernels = weights.shape[2]\n",
        "    out = np.zeros((x.shape[0], n_kernels), dtype=np.int64)\n",
        "    for i in range(n_kernels):\n",
        "        kernel = weights[:,:,i]\n",
        "        out[:,i] = conv1d_single_kernel(x, kernel, biases[i])\n",
        "    return out\n",
        "\n",
        "def max_pool_1d(x):\n",
        "    out = np.zeros((int(np.ceil(x.shape[0] / 2)), x.shape[1]), dtype=np.int8)\n",
        "    for j in range(x.shape[1]):\n",
        "        for i in range(int(np.floor(x.shape[0] / 2))):\n",
        "            out[i, j] = np.maximum(x[2*i, j], x[2*i+1, j])\n",
        "        if (x.shape[0] % 2 == 1):\n",
        "            out[-1, j] = x[-1, j]\n",
        "    return out\n",
        "\n",
        "def fc(x, weights, biases):\n",
        "    x = x.flatten()\n",
        "    out = np.matmul(x, weights, dtype=np.int64) + biases\n",
        "    return out\n",
        "\n",
        "def scale_feature_map(x, shift):\n",
        "    '''Scale a featuremap to a byte range given the amount to right shift by.\n",
        "\n",
        "    Since all weight, bias, and activation quantizations are mapped using the full range\n",
        "    of values, there is not any need for clamping the values to the range [-128, 127].\n",
        "    However, it is still useful since the quantization could be changed to not map to the\n",
        "    full range of the unquantized values.\n",
        "    '''\n",
        "    x = np.right_shift(x, shift)\n",
        "    x = np.clip(x, -128, 127)\n",
        "    x = x.astype(np.int8)\n",
        "    return x\n",
        "\n",
        "def get_numpy_pred(features):\n",
        "    out = np.zeros((features.shape[0], 2), dtype=np.int64)\n",
        "    for i in range(features.shape[0]):\n",
        "        # condition input feature map\n",
        "        x = features[i]\n",
        "        x = x.reshape((int(x.size / 13), 13))\n",
        "        x = np.clip(np.round(x * input_scale), -128, 127).astype(np.int8)\n",
        "\n",
        "        # conv1\n",
        "        x = conv1d_multi_kernel(x, conv1_weights, conv1_biases)\n",
        "        x = scale_feature_map(x, bitshifts[0])\n",
        "        x = max_pool_1d(x)\n",
        "\n",
        "        # conv2\n",
        "        x = conv1d_multi_kernel(x, conv2_weights, conv2_biases)\n",
        "        x = scale_feature_map(x, bitshifts[1])\n",
        "        x = max_pool_1d(x)\n",
        "\n",
        "        # fc1\n",
        "        x = fc(x, fc1_weights, fc1_biases)\n",
        "        \n",
        "        out[i] = x\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxDv67ZHvR3n",
        "outputId": "189efa83-9343-4234-df24-d0bd76702897"
      },
      "source": [
        "numpy_pred = get_numpy_pred(X[:1,:])\n",
        "numpy_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkgYmJvV7_Fa",
        "outputId": "8ae2c80c-546d-479a-fd42-ba13074acf4b"
      },
      "source": [
        "def get_torch_pred(input):\n",
        "    return net_quantized_with_bias.forward(torch.tensor(input, device=device, dtype=torch.float)).detach().cpu().numpy()\n",
        "\n",
        "torch_pred = get_torch_pred(X[:1,:])\n",
        "torch_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQPCZMWDRTGi",
        "outputId": "051f55fe-8e9e-4718-bf62-0ea7478eb8cd"
      },
      "source": [
        "numpy_pred - torch_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr6r5V359-4s",
        "outputId": "99e4f358-bb5a-4763-c4df-c0b1a903ed2c"
      },
      "source": [
        "# compare output for each sample in test data\n",
        "\n",
        "n_err = 0\n",
        "for i in range(Xtest.shape[0]):\n",
        "    numpy_pred = get_numpy_pred(X[i:i+1, :])\n",
        "    torch_pred = get_torch_pred(X[i:i+1, :])\n",
        "    diff = numpy_pred - torch_pred\n",
        "    if np.abs(diff).max() > 1e-9:\n",
        "        n_err += 1\n",
        "        print('Difference between outputs at sample ', i)\n",
        "    if n_err > 10:\n",
        "        break\n",
        "\n",
        "if n_err == 0:\n",
        "    print('No differences between numpy model and torch model!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No differences between numpy model and torch model!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Tsq1QbEyqA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yXTLUELLMex"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}